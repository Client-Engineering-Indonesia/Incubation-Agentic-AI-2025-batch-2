# Create Project in Langflow

### 1. Open Langflow

We have created Langflow deployments for participants to access. Feel free to open any one of these links:
- [Langflow Instance 1](#)
- [Langflow Instance 2](#)
- [Langflow Instance 3](#)

### 2. Create a Project

1. In Langflow, click **"Create First Project"** or **"New Project"**.
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 27 05" src="https://github.com/user-attachments/assets/8e29b353-7b9c-48a2-a9e6-4f420c105b69" />

2. From the template options, select **Vector Store RAG**
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 27 17" src="https://github.com/user-attachments/assets/65572f10-1101-421c-ba9f-ce75767eab49" />

> [!NOTE]
> ### â„¹ï¸ What is Vector Store RAG?
> 
> **Vector Store RAG (Retrieval-Augmented Generation)** is a pattern where:
> 
> - Input data (e.g., documents, PDFs) is converted into **embeddings** using an LLM.
> - These embeddings are stored in a **vector database** (like ChromaDB, Elasticsearch, Weaviate, etc.).
> - At query time, the system retrieves relevant vectors/documents based on similarity to the query and **injects them into the LLM context** to generate better answers.
> 
> This approach makes the agent more **knowledgeable**, **context-aware**, and **accurate**, especially in closed-domain tasks like answering enterprise-specific questions.

3. After selecting the template, rename your project to include your **name or initials**
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 40 02" src="https://github.com/user-attachments/assets/293bb04c-733b-4410-ac4f-a4273f27f557" />

### 3. Explore the Vector Store RAG Template
<img width="1162" height="721" alt="Screenshot 2025-07-17 at 17 19 53" src="https://github.com/user-attachments/assets/1393cb0e-7e1a-4dd8-a183-d534eb719a98" />

Once your project is created, you'll notice the template contains **two flows**:
#### ðŸ“š 1. Load Data Flow

This flow is responsible for:
- Taking input documents
- Splitting them into chunks
- Creating **embeddings** using OpenAI
- Storing those embeddings into a **vector database** (AstraDB in this default case)

#### ðŸ•  2. Retriever Flow

This is the "inference" flow:
- Accepts user questions
- Uses semantic similarity to **retrieve relevant chunks** from the vector database
- Combines retrieved context with the user question
- Sends it to an **LLM** to generate a response

### Customization for This Lab

In this lab, we will **customize the RAG flow** to use:

- **watsonx.ai** as the **LLM**
- **Elasticsearch** as the **vector store**
- create MCP server and use it in watsonx.Orchestrate

> [!NOTE]
> ### ðŸ¤– What is watsonx.ai?
> 
> **[watsonx.ai](https://www.ibm.com/watsonx/ai)** is IBMâ€™s enterprise-grade **AI studio** for building, training, and deploying **foundation models and machine learning models**.
> 
> #### ðŸ”§ Key Features:
> - Access to **open-source** and **proprietary foundation models**
> - Model tuning with labeled datasets
> - Prompt engineering and evaluation
> - Seamless integration with **watsonx.data**, **governance**, and **toolkits**
> - Designed for **regulated industries** (finance, healthcare, etc.)
> 
> In this lab, weâ€™ll use watsonx.ai to serve as the **LLM** that generates context-aware answers based on the userâ€™s input and retrieved data.

> [!NOTE]
> ### ðŸ” What is Elasticsearch?
> 
> **[Elasticsearch](https://www.elastic.co/elasticsearch/)** is a powerful, distributed search and analytics engine widely used for:
> 
> - Full-text search
> - Real-time log and data analytics
> - Semantic retrieval using **vector similarity search**
> 
> #### ðŸ§  Why Use It in RAG?
> 
> In the **RAG architecture**, Elasticsearch stores document embeddings and allows:
> 
> - Efficient **approximate nearest neighbor (ANN)** search
> - Fast retrieval of relevant context chunks
> - Integration with LLMs for semantic reasoning
> 
> It supports **dense vector fields**, making it compatible with modern LLM-based retrieval systems.
